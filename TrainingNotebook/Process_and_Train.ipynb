{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af9a160",
   "metadata": {},
   "source": [
    "# Combined Notebook: Animal Detection Data Processing and YOLOv10 Training\n",
    "\n",
    "This notebook combines the data processing and YOLOv10 training steps.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Import libraries, define paths, set up Kaggle download (optional).\n",
    "2.  **Dataset Download (Optional):** Download dataset from Kaggle.\n",
    "3.  **Data Loading & Filtering:** Load annotations, filter for relevant animals.\n",
    "4.  **Data Augmentation:** Define and apply augmentations to the training and validation sets.\n",
    "5.  **YOLO Preparation:** Convert annotations to YOLO format and create `data.yaml`.\n",
    "6.  **YOLOv10 Training:** Train the YOLOv10 model using the prepared dataset.\n",
    "7.  **Validation:** Evaluate the trained model on the validation set.\n",
    "8.  **Inference:** Run detection on a sample video.\n",
    "\n",
    "\n",
    "Dataset Located here - \n",
    "https://www.kaggle.com/datasets/goelyash/animal-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db72db",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b24326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: pandas in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: albumentations in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.3.71)\n",
      "Requirement already satisfied: kaggle in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (1.15.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (2.10.6)\n",
      "Requirement already satisfied: albucore==0.0.23 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (6.1.1)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: bleach in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (6.30.2)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (57.4.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (1.26.20)\n",
      "Requirement already satisfied: webencodings in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tomer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch torchvision torchaudio ultralytics pandas opencv-python matplotlib pyyaml albumentations tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ed04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 54\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import shutil\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import warnings\n",
    "#import kaggle\n",
    "\n",
    "# Suppress specific warnings from albumentations if needed (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='albumentations')\n",
    "\n",
    "# --- Configuration ---\n",
    "# !!! IMPORTANT: Update this path to your main project directory !!!\n",
    "base_dir = r'C:\\Users\\Tomer\\Documents\\DATASET\\Notebooks' # Use raw string for Windows paths\n",
    "\n",
    "# Define subdirectories relative to base_dir\n",
    "annotation_dir = os.path.join(base_dir, \"Annotation\")\n",
    "train_img_dir_orig = os.path.join(base_dir, \"Train\")  # Original Train images\n",
    "val_img_dir_orig = os.path.join(base_dir, \"Val\")    # Original Validation images\n",
    "aug_train_dir = os.path.join(base_dir, \"Augmented_Train\") # Augmented Train images\n",
    "aug_val_dir = os.path.join(base_dir, \"Augmented_Val\")   # Augmented Val images\n",
    "yolo_dir = os.path.join(base_dir, \"YOLO\")            # YOLOv10 specific files\n",
    "#kaggle_dataset_path = 'your-kaggle-username/your-dataset-name' # !!! IMPORTANT: Update this !!!\n",
    "#kaggle_download_dir = base_dir # Download directly into base_dir or a subdirectory\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(annotation_dir, exist_ok=True)\n",
    "os.makedirs(aug_train_dir, exist_ok=True)\n",
    "os.makedirs(aug_val_dir, exist_ok=True)\n",
    "os.makedirs(yolo_dir, exist_ok=True)\n",
    "\n",
    "# Define original annotation file paths\n",
    "train_ann_file_orig = os.path.join(annotation_dir, \"train_annotation.csv\")\n",
    "val_ann_file_orig = os.path.join(annotation_dir, \"val_annotation.csv\")\n",
    "\n",
    "# Define augmented annotation file paths\n",
    "aug_train_ann_file = os.path.join(annotation_dir, \"aug_train_annotation.csv\")\n",
    "aug_val_ann_file = os.path.join(annotation_dir, \"aug_val_annotation.csv\")\n",
    "\n",
    "# List of animals (classes) to monitor/detect\n",
    "animals_to_monitor = [\n",
    "    'African crocodile',\n",
    "    'African elephant',\n",
    "    'American alligator',\n",
    "    'American black bear',\n",
    "    'Arctic fox',\n",
    "    'baboon',\n",
    "    'badger',\n",
    "    'bear',\n",
    "    'beaver',\n",
    "    'bison',\n",
    "    'brown bear',\n",
    "    'capuchin',\n",
    "    'cheetah',\n",
    "    'cougar',\n",
    "    'coyote',\n",
    "    'crocodile',\n",
    "    'dingo',\n",
    "    'grey fox',\n",
    "    'hare',\n",
    "    'hog',\n",
    "    'hyena',\n",
    "    'ice bear',\n",
    "    'jaguar',\n",
    "    'Komodo dragon',\n",
    "    'leopard',\n",
    "    'lion',\n",
    "    'lynx',\n",
    "    'macaque',\n",
    "    'marmot',\n",
    "    'mink',\n",
    "    'Old World buffalo',\n",
    "    'otter',\n",
    "    'ox',\n",
    "    'porcupine',\n",
    "    'python',\n",
    "    'rabbit',\n",
    "    'ram',\n",
    "    'red fox',\n",
    "    'red wolf',\n",
    "    'skunk',\n",
    "    'sloth bear',\n",
    "    'snow leopard',\n",
    "    'squirrel',\n",
    "    'tiger',\n",
    "    'timber wolf',\n",
    "    'warthog',\n",
    "    'water buffalo',\n",
    "    'weasel',\n",
    "    'white wolf',\n",
    "    'wild boar',\n",
    "    'wild dog',\n",
    "    'wildcat',\n",
    "    'wolf',\n",
    "    'wood rabbit'\n",
    "]\n",
    "\n",
    "\n",
    "# Create mapping from animal name to class index\n",
    "class_map = {animal: i for i, animal in enumerate(animals_to_monitor)}\n",
    "print(f\"Number of classes: {len(class_map)}\")\n",
    "#print(\"Class Map:\", class_map) # Uncomment to view map\n",
    "\n",
    "# --- Training Configuration ---\n",
    "YOLO_MODEL_SIZE = 's' # e.g., 'n', 's', 'm', 'l', 'x'\n",
    "PRETRAINED_WEIGHTS_FILE = f'yolov10{YOLO_MODEL_SIZE}.pt'\n",
    "PRETRAINED_WEIGHTS_PATH = os.path.join(yolo_dir, PRETRAINED_WEIGHTS_FILE)\n",
    "EPOCHS = 100 # Adjust as needed\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce248b1",
   "metadata": {},
   "source": [
    "## 2. Dataset Download (Optional)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0facf0a-646c-4a91-bfa3-ba0a95222a22",
   "metadata": {},
   "source": [
    "CELL IS IN 'RAW' MODE AS IT IS NOT CURRENTLY NEEDED\n",
    "\n",
    "def download_kaggle_dataset(dataset_path, download_dir):\n",
    "    \"\"\"Downloads and extracts a dataset from Kaggle.\"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to download dataset '{dataset_path}' to '{download_dir}'...\")\n",
    "        # Ensure the target directory exists\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        \n",
    "        # Authenticate (requires kaggle.json in ~/.kaggle/ or environment variables)\n",
    "        kaggle.api.authenticate()\n",
    "        \n",
    "        # Download dataset files\n",
    "        kaggle.api.dataset_download_files(dataset_path, path=download_dir, unzip=True)\n",
    "        \n",
    "        print(f\"Dataset downloaded and extracted successfully to '{download_dir}'.\")\n",
    "        # You might need to add code here to move files from the downloaded structure \n",
    "        # (e.g., kaggle_download_dir/dataset_subfolder) to your expected locations \n",
    "        # (e.g., train_img_dir_orig, val_img_dir_orig, annotation_dir)\n",
    "        # Example (modify based on actual dataset structure):\n",
    "        # shutil.move(os.path.join(download_dir, 'train_images'), train_img_dir_orig)\n",
    "        # shutil.move(os.path.join(download_dir, 'val_images'), val_img_dir_orig)\n",
    "        # shutil.move(os.path.join(download_dir, 'train_annotations.csv'), train_ann_file_orig)\n",
    "        # shutil.move(os.path.join(download_dir, 'val_annotations.csv'), val_ann_file_orig)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset from Kaggle: {e}\")\n",
    "        print(\"Please ensure:\")\n",
    "        print(\"1. The 'kaggle' library is installed (`pip install kaggle`).\")\n",
    "        print(\"2. Your Kaggle API token (`kaggle.json`) is correctly placed (e.g., in `~/.kaggle/kaggle.json` or `C:\\Users\\<YourUsername>\\.kaggle\\kaggle.json`).\")\n",
    "        print(\"3. The dataset path is correct.\")\n",
    "\n",
    "# --- Set `download_from_kaggle` to True to execute download ---\n",
    "download_from_kaggle = False # <-- Set to True to run download\n",
    "\n",
    "if download_from_kaggle:\n",
    "    # !!! IMPORTANT: Replace with your actual Kaggle dataset path !!!\n",
    "    if kaggle_dataset_path == 'your-kaggle-username/your-dataset-name':\n",
    "        print(\"Skipping Kaggle download: Please update 'kaggle_dataset_path' variable.\")\n",
    "    else:\n",
    "        download_kaggle_dataset(kaggle_dataset_path, kaggle_download_dir)\n",
    "else:\n",
    "    print(\"Skipping Kaggle dataset download (download_from_kaggle is False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb4e4e",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Filtering\n",
    "\n",
    "Load the original annotation CSV files and filter them to include only images containing the `animals_to_monitor`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f2e1cdb-2655-4cc8-95ac-6f0a6e3ce9fa",
   "metadata": {},
   "source": [
    "def filter_valid_annotations(df, animals_list):\n",
    "    \"\"\"Filters DataFrame rows to keep only those with annotations for specified animals.\"\"\"\n",
    "    filtered_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        valid_animal_found = False\n",
    "        valid_data_for_row = {'File_location': row['File_location'], 'Width': row['Width'], 'Height': row['Height']}\n",
    "        num_cols = len(row)\n",
    "        max_annotations_per_image = (num_cols - 3) // 5 # Estimate max annotations based on columns like Name_i, Xmin_i, ...\n",
    "\n",
    "        for i in range(1, max_annotations_per_image + 1):\n",
    "            name_col = f'Name_{i}'\n",
    "            xmin_col = f'Xmin_{i}'\n",
    "            # Check if all necessary columns for this annotation exist\n",
    "            if name_col in row and xmin_col in row and pd.notna(row[name_col]):\n",
    "                if row[name_col] in animals_list:\n",
    "                    valid_animal_found = True\n",
    "                    # Add all related columns for this valid annotation\n",
    "                    valid_data_for_row.update({\n",
    "                        name_col: row[name_col],\n",
    "                        f'Xmin_{i}': row[f'Xmin_{i}'],\n",
    "                        f'Ymin_{i}': row[f'Ymin_{i}'],\n",
    "                        f'Xmax_{i}': row[f'Xmax_{i}'],\n",
    "                        f'Ymax_{i}': row[f'Ymax_{i}']\n",
    "                    })\n",
    "            else:\n",
    "                # Stop checking further annotations for this row if columns are missing\n",
    "                break \n",
    "                \n",
    "        if valid_animal_found:\n",
    "            filtered_data.append(valid_data_for_row)\n",
    "            \n",
    "    return pd.DataFrame(filtered_data)\n",
    "\n",
    "# Load original CSVs\n",
    "try:\n",
    "    train_df_orig = pd.read_csv(train_ann_file_orig, dtype=str, encoding='utf-8', low_memory=False)\n",
    "    val_df_orig = pd.read_csv(val_ann_file_orig, dtype=str, encoding='utf-8', low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Original annotation files not found at:\")\n",
    "    print(f\"- {train_ann_file_orig}\")\n",
    "    print(f\"- {val_ann_file_orig}\")\n",
    "    print(\"Please ensure the files exist or run the Kaggle download step.\")\n",
    "    # Optionally, raise an error or exit\n",
    "    # raise\n",
    "    train_df_orig, val_df_orig = pd.DataFrame(), pd.DataFrame() # Assign empty dfs\n",
    "\n",
    "if not train_df_orig.empty and not val_df_orig.empty:\n",
    "    # Apply filtering\n",
    "    train_df_filtered = filter_valid_annotations(train_df_orig, animals_to_monitor)\n",
    "    val_df_filtered = filter_valid_annotations(val_df_orig, animals_to_monitor)\n",
    "\n",
    "    print(\"Original Train shape:\", train_df_orig.shape)\n",
    "    print(\"Original Validation shape:\", val_df_orig.shape)\n",
    "    print(\"Filtered Train shape (relevant animals):\", train_df_filtered.shape)\n",
    "    print(\"Filtered Validation shape (relevant animals):\", val_df_filtered.shape)\n",
    "\n",
    "    # Display first few rows of filtered data\n",
    "    display(train_df_filtered.head())\n",
    "else:\n",
    "    print(\"Skipping filtering as original dataframes could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3479a4",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation\n",
    "\n",
    "Define augmentation pipelines using `albumentations` and apply them to the filtered training and validation datasets."
   ]
  },
  {
   "cell_type": "raw",
   "id": "444aa6eb-1492-4535-9bd0-a78399022486",
   "metadata": {},
   "source": [
    "# Define the augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    # Pixel-level transforms\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=25, val_shift_limit=10, p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.4),\n",
    "    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),\n",
    "    \n",
    "    # Noise and blur\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "    # A.ISONoise(color_shift=(0.01, 0.03), intensity=(0.1, 0.25), p=0.2), # Can be intense\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    A.MotionBlur(blur_limit=5, p=0.2),\n",
    "    \n",
    "    # Weather/Environmental effects (use cautiously, might overly obscure)\n",
    "    # A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.4, alpha_coef=0.1, p=0.15),\n",
    "    # A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=10, drop_width=1, blur_value=3, brightness_coefficient=0.9, p=0.15),\n",
    "    \n",
    "    # Dropout\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, min_holes=1, min_height=8, min_width=8, fill_value=0, p=0.3),\n",
    "    \n",
    "    # Basic geometric transforms (less aggressive)\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    # A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.3, border_mode=cv2.BORDER_CONSTANT), # Use if needed\n",
    "    \n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids'], min_visibility=0.1)) # 'pascal_voc' is [xmin, ymin, xmax, ymax]\n",
    "\n",
    "print(\"Augmentation pipeline defined.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50de0667-b611-416b-82e9-8066a26aef79",
   "metadata": {},
   "source": [
    "def apply_augmentations_and_save(df, img_dir_in, out_img_dir, num_aug=5):\n",
    "    \"\"\"Applies augmentations, saves images, and returns annotations.\"\"\"\n",
    "    augmented_annotations_list = []\n",
    "    processed_count = 0\n",
    "    total_images = len(df)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        img_filename_base = row['File_location']\n",
    "        # Construct the full path to the original image (assuming JPEG)\n",
    "        img_path = os.path.normpath(os.path.join(img_dir_in, img_filename_base + \".jpeg\"))\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            # Try without adding .jpeg if path includes it (handle potential inconsistency)\n",
    "            img_path_alt = os.path.normpath(os.path.join(img_dir_in, img_filename_base))\n",
    "            if os.path.exists(img_path_alt) and img_filename_base.lower().endswith('.jpeg'):\n",
    "                 img_path = img_path_alt\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {img_path} (or {img_path_alt}). Skipping row {index}.\")\n",
    "                continue\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Failed to load image: {img_path}. Skipping row {index}.\")\n",
    "            continue\n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Albumentations expects RGB\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Extract valid bounding boxes and category IDs from the row\n",
    "        bboxes = []\n",
    "        category_ids = []\n",
    "        num_cols = len(row)\n",
    "        max_annotations_per_image = (num_cols - 3) // 5 # Estimate\n",
    "        \n",
    "        for i in range(1, max_annotations_per_image + 1):\n",
    "            name_col = f'Name_{i}'\n",
    "            xmin_col, ymin_col, xmax_col, ymax_col = f'Xmin_{i}', f'Ymin_{i}', f'Xmax_{i}', f'Ymax_{i}'\n",
    "            \n",
    "            # Check if columns exist and contain data for this annotation index\n",
    "            if name_col in row and pd.notna(row[name_col]) and xmin_col in row and pd.notna(row[xmin_col]):\n",
    "                # Convert coordinates safely\n",
    "                try:\n",
    "                    xmin = float(row[xmin_col])\n",
    "                    ymin = float(row[ymin_col])\n",
    "                    xmax = float(row[xmax_col])\n",
    "                    ymax = float(row[ymax_col])\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"Warning: Invalid coordinate types in row {index}, annotation {i}. Skipping annotation. Error: {e}\")\n",
    "                    continue \n",
    "\n",
    "                # Clamp coordinates to image bounds and ensure validity\n",
    "                xmin = max(0, min(xmin, width))\n",
    "                ymin = max(0, min(ymin, height))\n",
    "                xmax = max(0, min(xmax, width))\n",
    "                ymax = max(0, min(ymax, height))\n",
    "                \n",
    "                if xmax > xmin and ymax > ymin: # Check for valid box dimensions\n",
    "                    bboxes.append([xmin, ymin, xmax, ymax])\n",
    "                    category_ids.append(row[name_col])\n",
    "                else:\n",
    "                     print(f\"Warning: Degenerate bbox found in row {index}, annotation {i} after clamping: {[xmin, ymin, xmax, ymax]}. Skipping annotation.\")\n",
    "            # else: # Optional: break if Name_i or Xmin_i is missing/NaN, assuming sequential annotations\n",
    "            #     break\n",
    "        \n",
    "        if not bboxes: # If no valid boxes were extracted for this image\n",
    "             print(f\"Warning: No valid bounding boxes found for image: {img_path} (row {index}). Skipping augmentation.\")\n",
    "             continue \n",
    "\n",
    "        # --- Save Original Image and Annotations --- \n",
    "        # Use a consistent naming scheme, e.g., replacing '/' with '_'\n",
    "        safe_filename_base = img_filename_base.replace('/', '_').replace('\\\\', '_')\n",
    "        orig_filename_jpeg = safe_filename_base + \"_orig.jpeg\"\n",
    "        orig_path = os.path.join(out_img_dir, orig_filename_jpeg)\n",
    "        cv2.imwrite(orig_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR)) # Save as BGR\n",
    "        \n",
    "        for bbox, cat in zip(bboxes, category_ids):\n",
    "            augmented_annotations_list.append({\n",
    "                \"File_location\": orig_filename_jpeg, # Use the actual saved filename\n",
    "                \"Width\": width,\n",
    "                \"Height\": height,\n",
    "                \"Name\": cat,\n",
    "                \"Xmin\": bbox[0],\n",
    "                \"Ymin\": bbox[1],\n",
    "                \"Xmax\": bbox[2],\n",
    "                \"Ymax\": bbox[3],\n",
    "            })\n",
    "\n",
    "        # --- Apply Augmentations --- \n",
    "        for i in range(num_aug):\n",
    "            try:\n",
    "                augmented = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "                aug_image = augmented['image']\n",
    "                aug_bboxes = augmented['bboxes']\n",
    "                aug_cat_ids = augmented['category_ids']\n",
    "\n",
    "                # Ensure image is in correct format for saving\n",
    "                aug_image_bgr = cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR)\n",
    "                aug_image_bgr = np.clip(aug_image_bgr, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                # Save augmented image\n",
    "                new_filename_jpeg = safe_filename_base + f\"_aug_{i}.jpeg\"\n",
    "                aug_path = os.path.join(out_img_dir, new_filename_jpeg)\n",
    "                cv2.imwrite(aug_path, aug_image_bgr)\n",
    "                \n",
    "                # Save augmented annotations\n",
    "                for bbox, cat in zip(aug_bboxes, aug_cat_ids):\n",
    "                     # Bbox coordinates should already be clamped by min_visibility in Compose\n",
    "                     # but an extra check can be added if needed\n",
    "                    augmented_annotations_list.append({\n",
    "                        \"File_location\": new_filename_jpeg, # Use the actual saved filename\n",
    "                        \"Width\": width, # Original width/height used for normalization later\n",
    "                        \"Height\": height,\n",
    "                        \"Name\": cat,\n",
    "                        \"Xmin\": bbox[0],\n",
    "                        \"Ymin\": bbox[1],\n",
    "                        \"Xmax\": bbox[2],\n",
    "                        \"Ymax\": bbox[3],\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during augmentation for image {img_path}, augmentation #{i}: {e}\")\n",
    "                # Decide whether to continue with next augmentation or skip image\n",
    "                continue \n",
    "                \n",
    "        processed_count += 1\n",
    "        if processed_count % 100 == 0:\n",
    "             print(f\"Processed {processed_count}/{total_images} images...\")\n",
    "\n",
    "    print(f\"Finished processing {processed_count}/{total_images} images.\")\n",
    "    return pd.DataFrame(augmented_annotations_list)\n",
    "\n",
    "# Check if filtered dataframes exist and are not empty\n",
    "if 'train_df_filtered' in locals() and not train_df_filtered.empty:\n",
    "    print(\"Starting Train Set Augmentation...\")\n",
    "    aug_train_df = apply_augmentations_and_save(train_df_filtered, train_img_dir_orig, aug_train_dir, num_aug=5)\n",
    "    aug_train_df.to_csv(aug_train_ann_file, index=False)\n",
    "    print(f\"Train augmentation completed. Augmented annotations saved to: {aug_train_ann_file}\")\n",
    "    print(f\"Augmented train annotations shape: {aug_train_df.shape}\")\n",
    "else:\n",
    "    print(\"Skipping train augmentation as filtered train dataframe is missing or empty.\")\n",
    "\n",
    "if 'val_df_filtered' in locals() and not val_df_filtered.empty:\n",
    "    print(\"\\nStarting Validation Set Augmentation...\")\n",
    "    # Apply fewer or no augmentations to validation set if desired (here applying same for simplicity)\n",
    "    aug_val_df = apply_augmentations_and_save(val_df_filtered, val_img_dir_orig, aug_val_dir, num_aug=1) # Only 1 'augmentation' (saving original)\n",
    "    aug_val_df.to_csv(aug_val_ann_file, index=False)\n",
    "    print(f\"Validation augmentation completed. Augmented annotations saved to: {aug_val_ann_file}\")\n",
    "    print(f\"Augmented validation annotations shape: {aug_val_df.shape}\")\n",
    "else:\n",
    "    print(\"Skipping validation augmentation as filtered validation dataframe is missing or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aca317",
   "metadata": {},
   "source": [
    "### Visualize an Example Augmented Image (Optional)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b869b2bb-69cd-42da-906b-fe277b29e72e",
   "metadata": {},
   "source": [
    "def plot_image_with_boxes(img_path, bboxes, labels):\n",
    "    \"\"\"Loads an image and draws bounding boxes.\"\"\"\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image {img_path}\")\n",
    "        return\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        try:\n",
    "            # Ensure coordinates are integers for drawing\n",
    "            x_min, y_min, x_max, y_max = map(int, map(float, bbox)) # Convert to float first for safety\n",
    "            # Draw rectangle\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2) # Red box\n",
    "            # Put label text\n",
    "            cv2.putText(image, str(label), (x_min, max(y_min - 10, 10)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "        except (ValueError, TypeError) as e:\n",
    "             print(f\"Error drawing box {bbox} with label {label}: {e}\")\n",
    "             continue\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.title(os.path.basename(img_path))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Check if augmented data exists\n",
    "if 'aug_train_df' in locals() and not aug_train_df.empty:\n",
    "    # Select a random augmented train image to display\n",
    "    try:\n",
    "        sample_row = aug_train_df.sample(1).iloc[0]\n",
    "        sample_img_filename = sample_row[\"File_location\"]\n",
    "        \n",
    "        # Find all annotations for this specific image file\n",
    "        sample_annotations = aug_train_df[aug_train_df[\"File_location\"] == sample_img_filename]\n",
    "        \n",
    "        sample_bboxes = sample_annotations[[\"Xmin\", \"Ymin\", \"Xmax\", \"Ymax\"]].values\n",
    "        sample_labels = sample_annotations[\"Name\"].tolist()\n",
    "        img_full_path = os.path.join(aug_train_dir, sample_img_filename)\n",
    "\n",
    "        print(f\"Displaying sample: {img_full_path}\")\n",
    "        plot_image_with_boxes(img_full_path, sample_bboxes, sample_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display sample image: {e}\")\n",
    "        print(\"Check if aug_train_df and image files exist.\")\n",
    "else:\n",
    "    print(\"Skipping visualization as augmented training data is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fb765",
   "metadata": {},
   "source": [
    "## 5. YOLO Preparation\n",
    "\n",
    "1.  **Convert CSV Annotations to YOLO Format:** Create a `.txt` file for each image with normalized bounding box coordinates (`class_id x_center y_center width height`).\n",
    "2.  **Organize Files:** Copy augmented images to the YOLO directory structure (`YOLO/train/images`, `YOLO/val/images`).\n",
    "3.  **Create `data.yaml`:** Generate the configuration file required by YOLO for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e55ba-c8d2-4aa0-a1ac-d356320eb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define YOLO directory structure paths\n",
    "yolo_train_images_dir = os.path.join(yolo_dir, \"train\", \"images\")\n",
    "yolo_train_labels_dir = os.path.join(yolo_dir, \"train\", \"labels\")\n",
    "yolo_val_images_dir = os.path.join(yolo_dir, \"val\", \"images\")\n",
    "yolo_val_labels_dir = os.path.join(yolo_dir, \"val\", \"labels\")\n",
    "data_yaml_path = os.path.join(yolo_dir, \"data.yaml\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for d in [yolo_train_images_dir, yolo_train_labels_dir, yolo_val_images_dir, yolo_val_labels_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"YOLO directory structure created.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddc0fc3e-3e47-4101-a008-da3743bec656",
   "metadata": {},
   "source": [
    "def convert_df_to_yolo_labels(df, labels_output_dir, class_mapping):\n",
    "    \"\"\"Converts annotations from DataFrame to YOLO format .txt files.\"\"\"\n",
    "    conversion_count = 0\n",
    "    skipped_count = 0\n",
    "    # Group annotations by image filename\n",
    "    for img_file, group in df.groupby(\"File_location\"):\n",
    "        # Define the label file path (replace image extension with .txt)\n",
    "        label_filename = os.path.splitext(img_file)[0] + \".txt\"\n",
    "        label_path = os.path.join(labels_output_dir, label_filename)\n",
    "        \n",
    "        lines = []\n",
    "        # Use the first row of the group to get image dimensions (should be same for all annotations of one image)\n",
    "        try:\n",
    "            img_w = float(group.iloc[0][\"Width\"])\n",
    "            img_h = float(group.iloc[0][\"Height\"])\n",
    "        except (ValueError, TypeError, KeyError) as e:\n",
    "             print(f\"Warning: Could not read Width/Height for {img_file}. Skipping. Error: {e}\")\n",
    "             skipped_count += 1\n",
    "             continue\n",
    "             \n",
    "        if img_w <= 0 or img_h <= 0:\n",
    "            print(f\"Warning: Invalid image dimensions ({img_w}x{img_h}) for {img_file}. Skipping.\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        for _, row in group.iterrows():\n",
    "            animal = row[\"Name\"]\n",
    "            if animal not in class_mapping:\n",
    "                # This shouldn't happen if filtering was done correctly, but good safety check\n",
    "                print(f\"Warning: Animal '{animal}' in {img_file} not in class map. Skipping annotation.\")\n",
    "                continue \n",
    "            \n",
    "            class_id = class_mapping[animal]\n",
    "            \n",
    "            try:\n",
    "                # Get box coordinates\n",
    "                xmin = float(row[\"Xmin\"])\n",
    "                ymin = float(row[\"Ymin\"])\n",
    "                xmax = float(row[\"Xmax\"])\n",
    "                ymax = float(row[\"Ymax\"])\n",
    "            except (ValueError, TypeError, KeyError) as e:\n",
    "                print(f\"Warning: Invalid coordinates for {animal} in {img_file}. Skipping annotation. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Normalize coordinates\n",
    "            x_center = ((xmin + xmax) / 2) / img_w\n",
    "            y_center = ((ymin + ymax) / 2) / img_h\n",
    "            bbox_width = (xmax - xmin) / img_w\n",
    "            bbox_height = (ymax - ymin) / img_h\n",
    "            \n",
    "            # Clamp values to be within [0.0, 1.0] and ensure width/height > 0\n",
    "            x_center = max(0.0, min(1.0, x_center))\n",
    "            y_center = max(0.0, min(1.0, y_center))\n",
    "            bbox_width = max(0.001, min(1.0, bbox_width)) # Avoid zero width/height\n",
    "            bbox_height = max(0.001, min(1.0, bbox_height))\n",
    "\n",
    "            # Create the YOLO format line\n",
    "            line = f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\"\n",
    "            lines.append(line)\n",
    "        \n",
    "        # Save the label file if there were valid annotations\n",
    "        if lines:\n",
    "            try:\n",
    "                with open(label_path, \"w\") as f:\n",
    "                    f.write(\"\\n\".join(lines))\n",
    "                conversion_count += 1\n",
    "            except IOError as e:\n",
    "                 print(f\"Error writing label file {label_path}: {e}\")\n",
    "                 skipped_count += 1\n",
    "        # else: # Optional: Print if an image ends up with no valid labels after processing\n",
    "        #    print(f\"Note: No valid labels generated for {img_file}\")\n",
    "            \n",
    "    print(f\"Finished YOLO label conversion: {conversion_count} files created, {skipped_count} images skipped.\")\n",
    "\n",
    "# Convert augmented annotations to YOLO format\n",
    "if 'aug_train_df' in locals() and not aug_train_df.empty:\n",
    "    print(\"Converting training annotations to YOLO format...\")\n",
    "    convert_df_to_yolo_labels(aug_train_df, yolo_train_labels_dir, class_map)\n",
    "else:\n",
    "     print(\"Skipping training label conversion: aug_train_df not found or empty.\")\n",
    "\n",
    "if 'aug_val_df' in locals() and not aug_val_df.empty:\n",
    "    print(\"Converting validation annotations to YOLO format...\")\n",
    "    convert_df_to_yolo_labels(aug_val_df, yolo_val_labels_dir, class_map)\n",
    "else:\n",
    "    print(\"Skipping validation label conversion: aug_val_df not found or empty.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ee7cf7b-ec1c-4755-8f5e-148cc5462573",
   "metadata": {},
   "source": [
    "def copy_images_to_yolo_dir(src_img_dir, dst_yolo_img_dir):\n",
    "    \"\"\"Copies image files from source to destination directory.\"\"\"\n",
    "    if not os.path.exists(src_img_dir):\n",
    "        print(f\"Warning: Source image directory not found: {src_img_dir}. Skipping copy.\")\n",
    "        return 0\n",
    "        \n",
    "    copied_count = 0\n",
    "    print(f\"Copying images from '{src_img_dir}' to '{dst_yolo_img_dir}'...\")\n",
    "    os.makedirs(dst_yolo_img_dir, exist_ok=True) # Ensure destination exists\n",
    "    \n",
    "    for filename in os.listdir(src_img_dir):\n",
    "        # Check if it's an image file (basic check)\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "            src_path = os.path.join(src_img_dir, filename)\n",
    "            dst_path = os.path.join(dst_yolo_img_dir, filename)\n",
    "            try:\n",
    "                shutil.copy2(src_path, dst_path) # copy2 preserves metadata\n",
    "                copied_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {filename}: {e}\")\n",
    "                \n",
    "    print(f\"Finished copying {copied_count} images.\")\n",
    "    return copied_count\n",
    "\n",
    "# Copy augmented images into the YOLO image directories\n",
    "copied_train = copy_images_to_yolo_dir(aug_train_dir, yolo_train_images_dir)\n",
    "copied_val = copy_images_to_yolo_dir(aug_val_dir, yolo_val_images_dir)\n",
    "\n",
    "if copied_train == 0:\n",
    "     print(f\"Warning: No training images were copied. Check source dir: {aug_train_dir}\")\n",
    "if copied_val == 0:\n",
    "     print(f\"Warning: No validation images were copied. Check source dir: {aug_val_dir}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98c4cfe2-10a3-4c77-a05a-e18525abcaeb",
   "metadata": {},
   "source": [
    "# Create the data.yaml file for YOLO training\n",
    "\n",
    "# Use absolute paths in data.yaml for robustness\n",
    "abs_yolo_train_images_dir = os.path.abspath(yolo_train_images_dir)\n",
    "abs_yolo_val_images_dir = os.path.abspath(yolo_val_images_dir)\n",
    "\n",
    "data_yaml_content = {\n",
    "    'train': abs_yolo_train_images_dir,\n",
    "    'val': abs_yolo_val_images_dir,\n",
    "    'nc': len(animals_to_monitor),    # Number of classes\n",
    "    'names': animals_to_monitor      # List of class names\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(data_yaml_path, 'w') as f:\n",
    "        yaml.dump(data_yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "    print(f\"data.yaml created successfully at: {data_yaml_path}\")\n",
    "    print(\"--- data.yaml content ---\")\n",
    "    print(yaml.dump(data_yaml_content))\n",
    "    print(\"-------------------------\")\n",
    "except IOError as e:\n",
    "     print(f\"Error writing data.yaml file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04ddda",
   "metadata": {},
   "source": [
    "## 6. YOLOv10 Training\n",
    "\n",
    "Load a pre-trained YOLOv10 model and fine-tune it on the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00f677-212d-41fd-bc21-7d1019e34114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Check GPU availability again\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available, training will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fb217-fda0-4c5a-ab99-ce7ed8ec3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained weights if they don't exist (using YOLO class)\n",
    "if not os.path.exists(PRETRAINED_WEIGHTS_PATH):\n",
    "    print(f\"Pre-trained weights '{PRETRAINED_WEIGHTS_FILE}' not found. Attempting to download...\")\n",
    "    try:\n",
    "        # This will download the weights if not found locally in standard paths\n",
    "        temp_model = YOLO(PRETRAINED_WEIGHTS_FILE) \n",
    "        # Check if download worked (ultralytics might place it elsewhere, find and move if necessary)\n",
    "        # Usually downloads to a cache dir, let's assume YOLO handles finding it\n",
    "        # If it needs to be explicitly in yolo_dir, add logic to find and move it.\n",
    "        print(f\"Downloaded {PRETRAINED_WEIGHTS_FILE} successfully (likely to cache). Training will use this.\")\n",
    "        # If you need it strictly in PRETRAINED_WEIGHTS_PATH, uncomment and adapt:\n",
    "        # downloaded_path = ... # Find where ultralytics saved it\n",
    "        # shutil.move(downloaded_path, PRETRAINED_WEIGHTS_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading pre-trained weights: {e}\")\n",
    "        print(\"Please download the weights manually and place them at:\")\n",
    "        print(PRETRAINED_WEIGHTS_PATH)\n",
    "        # Optionally raise error to stop execution\n",
    "        # raise\n",
    "        PRETRAINED_WEIGHTS_PATH = PRETRAINED_WEIGHTS_FILE # Fallback to just name if download failed\n",
    "else:\n",
    "    print(f\"Using existing pre-trained weights: {PRETRAINED_WEIGHTS_PATH}\")\n",
    "\n",
    "# Load the YOLOv10 model for training\n",
    "# Use the filename (e.g., 'yolov10s.pt') - YOLO handles loading\n",
    "model = YOLO(PRETRAINED_WEIGHTS_FILE) \n",
    "\n",
    "# Check if data.yaml exists\n",
    "if not os.path.exists(data_yaml_path):\n",
    "    print(f\"Error: data.yaml not found at {data_yaml_path}. Cannot start training.\")\n",
    "    # raise FileNotFoundError(f\"data.yaml not found at {data_yaml_path}\")\n",
    "else:\n",
    "    print(f\"Starting YOLOv10 training for {EPOCHS} epochs...\")\n",
    "    print(f\"Using data configuration: {data_yaml_path}\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    try:\n",
    "        # Start training\n",
    "        # Common arguments: data, epochs, imgsz, device, project, name, batch, patience, workers\n",
    "        results = model.train(\n",
    "            data=data_yaml_path,\n",
    "            epochs=EPOCHS,\n",
    "            imgsz=640, # Default, adjust if needed\n",
    "            device=DEVICE,\n",
    "            project=yolo_dir, # Save results within YOLO directory\n",
    "            name=f'yolov10_{YOLO_MODEL_SIZE}_custom_train', # Experiment name\n",
    "            exist_ok=True, # Allow overwriting previous runs with the same name\n",
    "            # batch=16, # Adjust based on GPU memory\n",
    "            # patience=20, # Early stopping patience\n",
    "            # workers=8 # Adjust based on CPU cores and system\n",
    "        )\n",
    "        print(\"\\nTraining completed!\")\n",
    "        print(f\"Model weights and results saved in: {results.save_dir}\")\n",
    "        # The best weights are usually saved as 'best.pt' in the experiment directory\n",
    "        best_model_path = os.path.join(results.save_dir, 'weights', 'best.pt') \n",
    "        print(f\"Best model saved at: {best_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        # You might want to investigate the error based on the traceback\n",
    "\n",
    "# Define path to the best trained model for later use\n",
    "# This assumes training ran successfully and results object is available\n",
    "try:\n",
    "    trained_model_path = os.path.join(results.save_dir, 'weights', 'best.pt')\n",
    "    if not os.path.exists(trained_model_path):\n",
    "         print(f\"Warning: Best model path {trained_model_path} not found after training. Validation/Inference might fail.\")\n",
    "         # Fallback or define manually if needed\n",
    "         trained_model_path = os.path.join(yolo_dir, f'yolov10_{YOLO_MODEL_SIZE}_custom_train', 'weights', 'best.pt') \n",
    "except NameError:\n",
    "     print(\"Warning: Training results object not found. Define 'trained_model_path' manually if needed for validation/inference.\")\n",
    "     # Define manually if training was skipped or failed\n",
    "     trained_model_path = os.path.join(yolo_dir, f'yolov10_{YOLO_MODEL_SIZE}_custom_train', 'weights', 'best.pt') # Adjust path if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac803298",
   "metadata": {},
   "source": [
    "## 7. Validation\n",
    "\n",
    "Evaluate the performance of the trained model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e11200-3d86-401b-b862-453d54e13ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Path to trained model for validation: {trained_model_path}\")\n",
    "\n",
    "if not os.path.exists(trained_model_path):\n",
    "    print(f\"Error: Trained model not found at {trained_model_path}. Cannot run validation.\")\n",
    "elif not os.path.exists(data_yaml_path):\n",
    "     print(f\"Error: data.yaml not found at {data_yaml_path}. Cannot run validation.\")\n",
    "else:\n",
    "    print(f\"\\nLoading trained model from {trained_model_path} for validation...\")\n",
    "    try:\n",
    "        # Load the best trained model\n",
    "        model = YOLO(trained_model_path)\n",
    "        \n",
    "        # Run validation\n",
    "        print(f\"Running validation using device: {DEVICE}...\") \n",
    "        # Note: You might want to use CPU ('cpu') for validation if GPU memory is limited after training\n",
    "        validation_results = model.val(\n",
    "            data=data_yaml_path,\n",
    "            imgsz=640,\n",
    "            device=DEVICE, \n",
    "            split='val', # Explicitly use the validation set\n",
    "            project=yolo_dir,\n",
    "            name=f'yolov10_{YOLO_MODEL_SIZE}_validation', # Separate folder for validation results\n",
    "            exist_ok=True \n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Validation Results Summary ---\")\n",
    "        # Access metrics through the results object (attributes might change slightly between ultralytics versions)\n",
    "        # Common metrics are typically in validation_results.box\n",
    "        map50_95 = getattr(validation_results.box, 'map', None) # mAP@0.5:0.95 (Primary metric)\n",
    "        map50 = getattr(validation_results.box, 'map50', None)   # mAP@0.5\n",
    "        # precision = getattr(validation_results.box, 'precision', None) # Precision\n",
    "        # recall = getattr(validation_results.box, 'recall', None)       # Recall\n",
    "        \n",
    "        if map50_95 is not None:\n",
    "            print(f\"mAP@0.5:0.95: {map50_95:.4f}\")\n",
    "        if map50 is not None:\n",
    "            print(f\"mAP@0.5:    {map50:.4f}\")\n",
    "        # if precision is not None:\n",
    "        #     print(f\"Precision:    {precision:.4f}\") # Usually available per class\n",
    "        # if recall is not None:\n",
    "        #     print(f\"Recall:       {recall:.4f}\")    # Usually available per class\n",
    "        \n",
    "        print(f\"\\nValidation metrics saved in: {validation_results.save_dir}\")\n",
    "        print(\"-------------------------------\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during validation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db395de",
   "metadata": {},
   "source": [
    "## 8. Inference on Video\n",
    "\n",
    "Run the trained model on a sample video to detect animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a149592-8d6e-4738-a3ad-3e9389b2ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_animals_in_video_ultralytics(model_path, input_video_path, output_video_path, \n",
    "                                       confidence_threshold=0.25, device='cpu'):\n",
    "    \"\"\"\n",
    "    Detects animals in a video using a trained YOLO model (via ultralytics)\n",
    "    and saves an annotated output video.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the trained YOLO model file (.pt).\n",
    "        input_video_path (str): Path to the input video file.\n",
    "        output_video_path (str): Path to save the annotated output video.\n",
    "        confidence_threshold (float): Minimum confidence score for detections.\n",
    "        device (str): Device to run inference on ('cpu', 'cuda:0', etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return\n",
    "        \n",
    "    if not os.path.exists(input_video_path):\n",
    "        print(f\"Error: Input video not found at {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the trained model\n",
    "        model = YOLO(model_path)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "\n",
    "        # Run inference on the video stream\n",
    "        print(f\"Starting inference on {input_video_path}...\")\n",
    "        # The predict method handles frame iteration, drawing, and saving\n",
    "        # Key args: source, save, show, conf, device, project, name, stream=True\n",
    "        results_generator = model.predict(\n",
    "            source=input_video_path,\n",
    "            save=False, # We will save manually for better control\n",
    "            stream=True, # Process video frame by frame\n",
    "            conf=confidence_threshold,\n",
    "            device=device,\n",
    "            verbose=False # Reduce console output during prediction\n",
    "        )\n",
    "\n",
    "        # --- Video Writer Setup ---\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Cannot open video capture for {input_video_path}\")\n",
    "            return\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "        print(f\"Output video setup: {output_video_path} ({width}x{height} @ {fps:.2f} FPS)\")\n",
    "        cap.release() # Release capture, ultralytics handles reading\n",
    "        # --- Process frames --- \n",
    "        frame_count = 0\n",
    "        for results in results_generator:\n",
    "            # The 'results' object contains detections for the current frame\n",
    "            # Use the built-in plot() method to get the annotated frame\n",
    "            annotated_frame = results.plot() \n",
    "            \n",
    "            # Write the annotated frame to the output video\n",
    "            out.write(annotated_frame)\n",
    "            frame_count += 1\n",
    "            if frame_count % 100 == 0:\n",
    "                 print(f\"Processed {frame_count} frames...\")\n",
    "            \n",
    "        # --- Clean Up ---\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"\\nFinished processing video.\")\n",
    "        print(f\"Annotated video saved to: {output_video_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during video inference: {e}\")\n",
    "        # Clean up resources if an error occurs mid-process\n",
    "        if 'out' in locals() and out.isOpened():\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Define paths for video inference\n",
    "input_video = os.path.join(base_dir, 'Wolves.mp4')   # !!! IMPORTANT: Update with your video file !!!\n",
    "output_video = os.path.join(base_dir, 'Analyzed_Wolves_ultralytics.mp4')\n",
    "\n",
    "# Check if input video exists\n",
    "if not os.path.exists(input_video):\n",
    "    print(f\"\\nInput video for inference not found: {input_video}\")\n",
    "    print(\"Please place your test video file at this location or update the 'input_video' path.\")\n",
    "elif not os.path.exists(trained_model_path):\n",
    "    print(f\"\\nTrained model path not found: {trained_model_path}\")\n",
    "    print(\"Cannot run video inference without a trained model.\")\n",
    "else:\n",
    "    # Run detection on the video\n",
    "    # Use the same device as training/validation or specify 'cpu'\n",
    "    detect_animals_in_video_ultralytics(\n",
    "        model_path=trained_model_path, \n",
    "        input_video_path=input_video, \n",
    "        output_video_path=output_video,\n",
    "        confidence_threshold=0.4, # Adjust confidence as needed\n",
    "        device=DEVICE # Use the globally defined device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc97137",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
